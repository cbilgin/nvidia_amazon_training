{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Recurrent Neural Networks\n",
    "\n",
    "\n",
    "In our first challenge, we created a model that predicted labels from raw pixels, generating a high-level understanding of what the image represents. In this section we'll create a model that predicts the next word of a sentence from previous words, generating an understanding of language.\n",
    "\n",
    "We exposed a network that was designed to learn the difference between images to a dataset full of labeled images. Here, we'll expose a network that is designed to learn about the structure of language to a **corpus** of text.\n",
    "\n",
    "After this notebook, we'll have one [model](#model \\\"a trained network\\\") that understands images and another that understands language. We'll combine them to generate captions from images and video. \n",
    "\n",
    "Let's learn how to teach a neural network about language. We'll start with a small example using a tiny subset of the English language and a small corpus comprised of two sentences which will represent everything our network will know about language.\n",
    "\n",
    "First, our dictionary (Click the cell below and press Shift and Enter to execute the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_dict=['EOS','a','my','sleeps','on','dog','cat','the','bed','floor'] #'EOS' means end of sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create our small corpus that our network can use to learn about language. Let's create a few sentences from the words in our dictionary. The first vector in the numpy array 'X' represents the sentence ['my','cat','sleeps','on','my','bed', 'EOS']. Use that as a model to replace the ##FIXME## vector with the sentence, ['a', 'dog', 'sleeps', 'on', 'the', 'floor', 'EOS']. Hover [here](#hint \"The second line should be X=np.array([[2,6,3,4,2,8,0],[1,5,3,4,7,9,0]]),dtype=np.int32)\") for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #numpy is \"numerical python\" and is used in deep learning mostly for its n-dimensional array\n",
    "X=np.array([[2,6,3,4,2,8,0],[##FIXME##]],dtype=np.int32) \n",
    "print([small_dict[ind] for ind in X[1,:]]) #Feel free to change 1 to 0 to see the other sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we need to structure it for our network. Words are fundamentally different types of data from images. A 28X28 grayscale *image* is viewed as a 28X28 matrix where each cell represents the \"grayness\" of that pixel. A 256X256 color image is viewed as a 256X256X3 [tensor](#tensor \"a vector, matrix, or any other \"block\" of n-dimensional data) where each cell contains the \"redness,\" \"greenness,\" and \"blueness\" at each pixel.\n",
    "\n",
    "When classifying images, we used TensorFlow (or any other framework) to describe how that input tensor flows to a vector of probabilities. \n",
    "\n",
    "Words must also be converted into tensors before we can use them as our input. For this example, we'll use \"one-hot-encoding,\" where each word will be represented by a vector with one '1' and the rest 0s. The vectors will be the length of the dictionary to have a '1' in a unique place depending on the word.\n",
    "\n",
    "![](one-hot.PNG)\n",
    "\n",
    "To see what we mean, let's run a TensorFlow [session](#sess \\\"where computational graphs described in TensorFlow are run\\\") to transform our input data to one-hot encoding and visualize what that looks like. Tensorflow's embedding_lookup and unstack functions make it easy to do this. For one-hot encoding, we can pass embedding_lookup an [identity matrix](#idmat \"A matrix with ones in the diagonal and zeros everywhere else\") the length of our dictionary and our input dataset. This could be done as part of a training session, but we've seperated it here to visualize our inputs. \n",
    "\n",
    "As a reminder of how small a subset of the English language we're using, replace ##FIXME## with the length of our dictionary. Hover [here](#dict_length \"Replace ##FIXME## in np.identity with len(small_dict).\") for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "tf.reset_default_graph()\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        result=tf.nn.embedding_lookup(np.identity(##FIXME##), X).eval()\n",
    "        example_input=sess.run([tf.unstack(result,X.shape[1],1)])\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "print('one-hot encoded inputs')\n",
    "print(result)\n",
    "print('shape of the input')\n",
    "print(result.shape)\n",
    "print('reshaped input for training')\n",
    "print(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our data! \n",
    "\n",
    "Recall that neural networks start out as randomized values. Through exposure to data, networks \"learn\" to create accurate mappings from inputs to outputs. \n",
    "\n",
    "Our image classification network output label predictions from image inputs. Our language processing network will output *next word* predictions from input of previous words. \n",
    "\n",
    "For example, let's take a sentence and attempt to do this ourselves (try not to look ahead):\n",
    "\n",
    "Our sentence starts with the word: \"My\"\n",
    "\n",
    "Using the dictionary of all the words you know, and the patterns that you have observed throughout your life, what word might come after the word \"My\"?\n",
    "\n",
    "Great, now that you've guessed, you find out that the second word is \"friend\".\n",
    "\n",
    "You learn a bit more about this sentence, and can use the first TWO words to guess the third. \n",
    "\n",
    "Next, you learn the third word is \"went\", you take that error and guess again. Do you know more about the structure of this sentence than you did at the beginning? If this were the first or thousandth sentence you've seen, you likely also know more about the structure of *all* sentences.\n",
    "\n",
    "This is one way networks learn about language. This type of network is called a **recurrent neural network (RNN)**. They can learn all kinds of conventions: how \"subjects\" are related to \"verbs,\" when punctuation usually occurs, etc, if they have enough time and a large enough dataset. They learn by reducing the error between their predicted next word and the actual next word in a corpus. RNNs are structured to \"remember\" the words that led to their prediction. \n",
    "\n",
    "Using the simplest RNN that we can, with only one layer, let's see what we can learn from our two sentences about the relationship between our 8 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_loss=[]\n",
    "num_hidden=24\n",
    "num_steps=X.shape[1]\n",
    "dict_length=len(small_dict)\n",
    "batch_size=2\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Make Variables\n",
    "variables_dict = {\n",
    "    \"weights1\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],stddev=1.0,dtype=tf.float32),name=\"weights1\"),\n",
    "    \"biases1\": tf.Variable(tf.truncated_normal([dict_length],stddev=1.0,dtype=tf.float32), name=\"biases1\")}\n",
    "\n",
    "# Create input data\n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), X) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "y=np.zeros((batch_size,num_steps),dtype=np.int32)\n",
    "y[:,:-1]=X[:,1:]\n",
    "y_one_hot=tf.unstack(tf.nn.embedding_lookup(np.identity(dict_length), y),num_steps,1) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "\n",
    "y_target_reshape=tf.reshape(y_one_hot,[batch_size*num_steps,dict_length])\n",
    "\n",
    "#Create our LSTM\n",
    "cell = tf.contrib.rnn.LSTMCell(num_units=num_hidden, state_is_tuple=True)\n",
    "\n",
    "outputs, last_states = tf.contrib.rnn.static_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=tf.unstack(tf.to_float(X_one_hot),num_steps,1))\n",
    "\n",
    "output_reshape=tf.reshape(outputs, [batch_size*num_steps,num_hidden]) #[12==batch_size*num_steps,num_hidden==12]\n",
    "pred=tf.matmul(output_reshape, variables_dict[\"weights1\"]) +variables_dict[\"biases1\"]\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target_reshape))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "plot_loss=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)        \n",
    "        for i in range(10):\n",
    "            loss,_,y_target,y_pred,output=sess.run([cost,optimizer,y_target_reshape,pred,outputs])\n",
    "            plot_loss.append([loss])\n",
    "\n",
    "            if i% 5 ==0:\n",
    "                print(\"iteration: \",i,\" loss: \",loss)\n",
    "                \n",
    "        print(y_target)\n",
    "        print(np.argmax(y_pred,1))          \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RNN has seen our 2 sentences 10 times. Each time it has seen a new word, it has attempted to predict the next word. TensorFlow is reporting a \"loss\" or error in those predictions of ~2.15. Let's see what this looks like with a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets look at one input data point at each step and its prediction\n",
    "print(\"Input Sentence\")\n",
    "sn=0 #The sentence number\n",
    "print([small_dict[ind] for ind in X[sn,:]])\n",
    "print(\"Predicted words\")\n",
    "print([small_dict[ind] for ind in np.argmax(y_pred[sn::2],1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we at least predict the sentences used for training?  By passing the first word of the sentence to the predictor, the result should be our original sentence.  What happened?\n",
    "\n",
    "Try raising the number of iterations in the code block above to 300.  How does this affect the ability of the network to predict the original sentences?\n",
    "\n",
    "While we're able to perfectly predict sentences in our small sample, we're going to take on more complex examples next. Let's peek at some levers that we have that can affect performance: the depth of the network and an operation called \"dropout.\" \n",
    "\n",
    "Deeper models have the ability to represent more complex functions. To build a deeper model in TensorFlow, we can just create our layers in a loop.\n",
    "\n",
    "Let's train an RNN with 2 and 4 layers. What parameter do you need to set to change the number of layers in your RNN? For a hint hover [here](#answer1 \"num_layers=2 or num_layer=4. This is used in the 'for' loop where lstm_cells are created\").\n",
    "\n",
    "Dropout increases your model's ability to generalize by actually asking it to \"forget\" some parameters as it trains. To see where you adjust dropout values, hover [here](#answer2 \"dropout = ___, where 1.0 = none -remember everything and 0.0 = all -remember nothing\". This is implemented here: lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,input_keep_prob=dropout,output_keep_prob=dropout)\"). \n",
    "\n",
    "Experiment with the two to see if you can increase performance.                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's try multiple layers \n",
    "plot_loss2=[]\n",
    "num_hidden=24\n",
    "num_steps=X.shape[1]\n",
    "dict_length=len(small_dict)\n",
    "batch_size=2\n",
    "num_layers=##FIXME##\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Make Variables\n",
    "variables_dict = {\n",
    "    \"weights1\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],stddev=1.0,dtype=tf.float32),name=\"weights1\"),\n",
    "    \"biases1\": tf.Variable(tf.truncated_normal([dict_length],stddev=1.0,dtype=tf.float32), name=\"biases1\")}\n",
    "\n",
    "\n",
    "# Create input data\n",
    "#small_dict=['EOS','i','will','walk','the','dog','cat','run']\n",
    "#X=np.array([[1,2,7,4,5,0],[1,2,3,4,6,0]],dtype=np.int32)  \n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), X) \n",
    "y=np.zeros((batch_size,num_steps),dtype=np.int32)\n",
    "y[:,:-1]=X[:,1:]\n",
    "y_one_hot=tf.unstack(tf.nn.embedding_lookup(np.identity(dict_length), y),num_steps,1) \n",
    "y_target_reshape=tf.reshape(y_one_hot,[batch_size*num_steps,dict_length])\n",
    "dropout = ##FIXME##\n",
    "\n",
    "\n",
    "##################### Create a multilayer RNN ####################\n",
    "layer_cell=[]\n",
    "for _ in range(num_layers):\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_hidden, state_is_tuple=True)\n",
    "    lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,\n",
    "                                          input_keep_prob=dropout,\n",
    "                                          output_keep_prob=dropout)\n",
    "    layer_cell.append(lstm_cell)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(layer_cell, state_is_tuple=True)\n",
    "outputs, last_states = tf.contrib.rnn.static_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=tf.unstack(tf.to_float(X_one_hot),num_steps,1))\n",
    "\n",
    "output_reshape=tf.reshape(outputs, [batch_size*num_steps,num_hidden])\n",
    "pred=tf.matmul(output_reshape, variables_dict[\"weights1\"]) +variables_dict[\"biases1\"]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target_reshape))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(300):\n",
    "            loss,_,y_target,y_pred,output=sess.run([cost,optimizer,y_target_reshape,pred,outputs])\n",
    "            plot_loss2.append([loss])\n",
    "            \n",
    "            if i% 25 ==0:\n",
    "                print(\"iteration: \",i,\" loss: \",loss)\n",
    "                \n",
    "        print(y_target)\n",
    "        print(np.argmax(y_pred,1))         \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the loss from our single and multi-layer RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(plot_loss,'r.')\n",
    "plt.plot(plot_loss2,'b--')\n",
    "plt.legend([\"1-Layer LSTM\",\"Multi-Layer LSTM\"])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.title(\"Loss During Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model can complete these two sentences well, but knows nothing about the rest of language. (It's like we learned two lines of a song.) If, instead, we were working to build an understanding of language in general, we'd be heavily **overfitting** to our corpus. Increasing the complexity (depth) of our model can learn about more complex [corpuses](#corpora \"actually corpora\") but can cause overfitting. Dropout is one method for reducting overfitting. Let's keep these strategies in mind as we work to learn about an entire type of language, the caption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an RNN with MSCOCO Captions \n",
    "Now we are going to use the [Microsoft Common Objects in Context](http://mscoco.org/) (MSCOCO) image captions to train an RNN to understand captions. The cell below shows one way to read, format, and feed the data into TensorFlow. First, we will read the caption file, then we will remove the punctuation, and then train. Due to time constraints, we won't use the full dataset for this training. However, it would be easy to change this and train with more or the entire dataset. Can you see an easy way to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "#import reader\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "num_steps=20\n",
    "## Read Training files\n",
    "with open(\"/data/mscoco/captions_train2014.json\") as data_file:\n",
    "         data=json.load(data_file)\n",
    "\n",
    "TotalNumberofCaptions=len(data['annotations'])\n",
    "\n",
    "sentences=[]\n",
    "\n",
    "##Create a list of all of the sentences.\n",
    "for i in range(TotalNumberofCaptions):\n",
    "        sentences+=[re.sub('[^A-Za-z0-9]+',' ',data['annotations'][i]['caption']).lower()]\n",
    "\n",
    "TotalWordList=[]\n",
    "for i in range(TotalNumberofCaptions):\n",
    "        TotalWordList+=re.sub('[^A-Za-z0-9]+',' ',data['annotations'][i]['caption']).lower().split()\n",
    "\n",
    "#Determine number of distinct words \n",
    "distinctwords=collections.Counter(TotalWordList)\n",
    "#Order words \n",
    "count_pairs = sorted(distinctwords.items(), key=lambda x: (-x[1], x[0])) #ascending order\n",
    "words, occurence = list(zip(*count_pairs))\n",
    "DictionaryLength=occurence.index(4) #index for words that occur 4 times or less\n",
    "words=['PAD','UNK','EOS']+list(words[:DictionaryLength])\n",
    "word_to_id=dict(zip(words, range(len(words))))\n",
    "#Tokenize Sentence\n",
    "Tokenized=[]\n",
    "for full_words in sentences:\n",
    "        EmbeddedSentence=[word_to_id[word] for word in full_words.split() if word in word_to_id]+[word_to_id['EOS']]\n",
    "        #Pad sentences that are shorter than the number of steps \n",
    "        if len(EmbeddedSentence)<num_steps:\n",
    "            b=[word_to_id['PAD']]*num_steps\n",
    "            b[:len(EmbeddedSentence)]=EmbeddedSentence\n",
    "        if len(EmbeddedSentence)>num_steps:\n",
    "            b=EmbeddedSentence[:num_steps]\n",
    "        if len(b)==EmbeddedSentence:\n",
    "            b=EmeddedSentence\n",
    "        b=[word_to_id['UNK'] if x>=DictionaryLength else x for x in b] #turn all words used 4 times or less to 'UNK'\n",
    "        #print(b)\n",
    "        Tokenized+=[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we take these captions and \"tokenize\" them, or convert each word into a number (in descending order based on popularity). We also have a variety of sentence lengths, so in order to create a standard input and output tensor, we've padded short sentence with 0s and truncated long sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################# Parameters #####################################################\n",
    "\n",
    "num_hidden=256\n",
    "num_steps=20\n",
    "dict_length=len(words)\n",
    "batch_size=4\n",
    "\n",
    "\n",
    "## Create labels\n",
    "Label=[]\n",
    "for caption in Tokenized:\n",
    "    Label+=[caption[1:]+[word_to_id['PAD']]]\n",
    "\n",
    "NumberofCasestoEvaluate=20\n",
    "TrainingInputs=Tokenized[:NumberofCasestoEvaluate]\n",
    "LabelInputs=Label[:NumberofCasestoEvaluate]\n",
    "\n",
    "#Print out some variables \n",
    "print(TrainingInputs[0])\n",
    "print(LabelInputs[0])\n",
    "print(\"Number of words in this dictionary\", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see two things:\n",
    "\n",
    "1. Our [labels](#labels \"labels are the outputs that we want our network to generate\") are the next *token* in our training set.\n",
    "2. This dictionary is much bigger. \n",
    "\n",
    "We'll feed our data to our network in batches of 4 to take further advantage of parallel processing and the GPU in training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create our input queue\n",
    "def data_input_queue(TrainingInputs, LabelInputs, num_steps):\n",
    "    train_input_queue = tf.train.slice_input_producer(\n",
    "                                    [TrainingInputs, LabelInputs],\n",
    "                                    shuffle=True)\n",
    "\n",
    "    ##Set our train data and label input shape for the queue\n",
    "    TrainingInput=train_input_queue[0]\n",
    "    LabelInput=train_input_queue[1]\n",
    "    TrainingInput.set_shape([num_steps])\n",
    "    LabelInput.set_shape([num_steps])\n",
    "    min_after_dequeue=100000\n",
    "    capacity = min_after_dequeue + 3 * batch_size \n",
    "    #input_x, target_y\n",
    "    train_x, train_y = tf.train.batch([TrainingInput, LabelInput],\n",
    "                                                 batch_size=batch_size ,\n",
    "                                                 capacity=capacity,\n",
    "                                                 num_threads=4)\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to train an RNN with the MSCOCO captions. Feel free to experiment with number of layers and dropout again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_layers=1\n",
    "dropout = 1.0\n",
    "\n",
    "loss_mscoco=[]\n",
    "#######################################################################################################\n",
    "TrainingInputs=Tokenized[:NumberofCasestoEvaluate]\n",
    "LabelInputs=Label[:NumberofCasestoEvaluate]\n",
    "\n",
    "\n",
    "variables_dict = {\n",
    "    \"weights_mscoco\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32),name=\"weights_mscoco\"),\n",
    "    \"biases_mscoco\": tf.Variable(tf.truncated_normal([dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32), name=\"biases_mscoco\")}\n",
    "\n",
    "\n",
    "# Create input data\n",
    "train_x, train_y =data_input_queue(TrainingInputs, LabelInputs, num_steps)\n",
    "mscoco_dict=words\n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), train_x) #[batch,num_steps,dictionary_length]\n",
    "y_one_hot=tf.unstack(tf.nn.embedding_lookup(np.identity(dict_length), train_y),num_steps,1)#[batch,num_steps,dictionary_length]\n",
    "y_target_reshape=tf.reshape(y_one_hot,[batch_size*num_steps,dict_length])\n",
    "\n",
    "input_keep_prob=dropout\n",
    "output_keep_prob=dropout\n",
    "\n",
    "#Create a multilayer RNN\n",
    "\n",
    "layer_cell=[]\n",
    "for _ in range(num_layers):\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_hidden, state_is_tuple=True)\n",
    "    ############# add dropout #########################\n",
    "    lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,\n",
    "                                          input_keep_prob=dropout,\n",
    "                                          output_keep_prob=dropout)\n",
    "    layer_cell.append(lstm_cell)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(layer_cell, state_is_tuple=True)\n",
    "outputs, last_states = tf.contrib.rnn.static_rnn(\n",
    "    cell=lstm_cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=tf.unstack(tf.to_float(X_one_hot),num_steps,1))\n",
    "\n",
    "output_reshape=tf.reshape(outputs, [batch_size*num_steps,num_hidden])\n",
    "pred=tf.matmul(output_reshape, variables_dict[\"weights_mscoco\"]) +variables_dict[\"biases_mscoco\"]\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target_reshape))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(cost,aggregation_method = tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        for i in range(1500):\n",
    "            x_input,y_input=sess.run([train_x, train_y])\n",
    "            loss,_,y_target,x_input,y_input,y_pred=sess.run([cost,optimizer,y_target_reshape,train_x, train_y,pred])\n",
    "            loss_mscoco.append([loss])\n",
    "            if i% 100==0:\n",
    "                print(\"iteration: \",i, \"loss: \",loss)  \n",
    "        print(\"Done Training\")\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets look at one input data point and its prediction\n",
    "print(\"Input Sentence\")\n",
    "batch_element=2\n",
    "print([words[ind] for ind in x_input[batch_element,:]])\n",
    "print(\"Target\")\n",
    "print([words[ind] for ind in y_input[batch_element,:]])\n",
    "print(\"Predicted words\")\n",
    "print([words[ind] for ind in np.argmax(y_pred[batch_element::batch_size],1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How'd we do? We've learned to use RNNs to predict next words from previous words and our RNN's understanding of language. \n",
    "\n",
    "What kind of problems could we solve by deploying this model?\n",
    "\n",
    "If we built the skills to do this really well, we could do interesting things like start to mimic someone's style of writing, generate stock market quote predictions based on past performance, or suggest next words in text messages.\n",
    "\n",
    "We're limited by the fact that the [ground truth](#gt \"actual next word at every timestep\") is being propogated through the network. If we were to generate more than just a single word, our performance would be dismal. In the next section, we're going to provide our network with a muse, something to write about, in the form of an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to combine our CNN's high-level understanding of the image AND our RNN's understanding of captions to generate captions from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Free our GPU memory before proceeding to the next part of the lab - you'll get a warning indicating \"Kernal is dead.\" That's to be expected. \n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References \n",
    "[1] Imanol Schlab. TensorFLow Input Pipeline Example. http://ischlag.github.io/2016/06/19/tensorflow-input-pipeline-example/\n",
    "\n",
    "[2] Denny Britz. Practical Examples for RNNs in TensorFlow https://github.com/dennybritz/tf-rnn\n",
    "\n",
    "[3]Lin, Tsung-Yi, et al. \"Microsoft coco: Common objects in context.\" European Conference on Computer Vision. Springer International Publishing, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
