{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning\n",
    "\n",
    "In this lab, we are going to combine deep learning workflows to generate descriptions of scenes with AI. The Microsoft Common Object in Context (MSCOCO) data set is comprised of many images and five captions for each image, so we've got an input and a corresponding desired output, the two required ingredients for supervised learning. Our challenge will be identifying the right network workflow given both visual and language based data.\n",
    "\n",
    "With image classification, we learned how to generate a high-level understanding of an image. With the basics of natural language processing, we learned how to use recurrent neural networks (RNNs) to generate text from context. In this lab, we'll learn to add our high-level understanding of images to the context that our RNNs uses to generate language predictions. \n",
    "\n",
    "Let's start by importing modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "#import reader\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "import sys\n",
    "sys.path.insert(0, '/data/models/slim')\n",
    "\n",
    "slim=tf.contrib.slim\n",
    "from nets import vgg\n",
    "\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "%matplotlib inline  \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixels to Context\n",
    "\n",
    "Let's first examine a way to modify an image classification workflow to generate a high-level understanding of an image that retains more information than a simple label.\n",
    "\n",
    "We will feed the images from MSCOCO through VGG, another award-winning image classification network. Instead of viewing VGGs \"softmax\" or prediction layer output, we're going to view the layer just before. This is a [feature vector](#fv \"An array midway through the neural network that represents some features of the input\") called fc7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_PATH='/data/mscoco/train2014/'\n",
    "## Read Training files\n",
    "with open(\"/data/mscoco/captions_train2014.json\") as data_file:\n",
    "         data=json.load(data_file)\n",
    "\n",
    "image_feature_vectors={}   \n",
    "tf.reset_default_graph()\n",
    "    \n",
    "one_image=ndimage.imread(TRAIN_IMAGE_PATH+data[\"images\"][0]['file_name'])\n",
    "    #resize for vgg network\n",
    "resize_img=misc.imresize(one_image,[224,224])\n",
    "if len(one_image.shape)!= 3: #Check to see if the image is grayscale if True mirror colorband\n",
    "    resize_img=np.asarray(np.dstack((resize_img, resize_img, resize_img)), dtype=np.uint8)\n",
    "\n",
    "processed_image = vgg_preprocessing.preprocess_image(resize_img, 224, 224, is_training=False)\n",
    "processed_images  = tf.expand_dims(processed_image, 0)      \n",
    "network,endpts= vgg.vgg_16(processed_images, is_training=False)\n",
    "\n",
    "   \n",
    "init_fn = slim.assign_from_checkpoint_fn(os.path.join('/data/mscoco/vgg_16.ckpt'),slim.get_model_variables('vgg_16'))\n",
    "sess = tf.Session()\n",
    "init_fn(sess)\n",
    "NETWORK,ENDPTS=sess.run([network,endpts])\n",
    "sess.close()\n",
    "print('fc7 array for a single image')\n",
    "print(ENDPTS['vgg_16/fc7'][0][0][0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear there's a lot of information there. Let's find out what's contained in the \"...\" by visualizing the vector as a plot instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ENDPTS['vgg_16/fc7'][0][0][0])\n",
    "plt.xlabel('feature vector index')\n",
    "plt.ylabel('amplitude')\n",
    "plt.title('fc7 feature vector')\n",
    "data[\"images\"][0]['file_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this plot might not mean much to us, it is the result of a neural network that was designed to extract features from pixel data. We will use this vector as \"context\" for our caption generation. \n",
    "\n",
    "Before we do that, experiment with the code block below to see the response of different layers in your network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you look at feature maps from the first convolutional layer? Look here if you need a [hint](#answer1 \"The output from the convolutional layer is in the form of height, width, and number of feature maps. FEATUREMAPID can be any value between 0 and the number of feature maps minus 1.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featuremap='vgg_16/conv1/conv1_1'\n",
    "print(ENDPTS[featuremap][0].shape)\n",
    "FEATUREMAPID=63\n",
    "print('input image and feature map from ',featuremap)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(resize_img)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(ENDPTS[featuremap][0][:,:,FEATUREMAPID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you look at the response of different layers in your network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning captions with images\n",
    "\n",
    "Next, we are going to combine the feature maps with their respective captions. Many of the images have five captions. Run the code below to view the captions for one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CaptionsForOneImage=[]\n",
    "for k in range(len(data['annotations'])):\n",
    "    if data['annotations'][k]['image_id']==data[\"images\"][0]['id']:\n",
    "        CaptionsForOneImage.append([data['annotations'][k]['caption'].lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(resize_img)\n",
    "print('MSCOCO captions for a single image')\n",
    "CaptionsForOneImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of time, we've already created a file with feature vectors from 2000 of the MSCOCO images has been created. Next, you will load and align these with captions. Please note this step can take more than 5 minutes to run.  While you wait, take some time to read about the MSCOCO data set.  The main website is here (www.mscoco.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_load=np.load('/data/mscoco/train_vgg_16_fc7_2000.npy').tolist()\n",
    "image_ids=example_load.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create 3 lists image_id, feature maps, and captions.\n",
    "image_id_key=[]\n",
    "feature_maps_to_id=[]\n",
    "caption_to_id=[]\n",
    "for observed_image in image_ids:   \n",
    "    for k in range(len(data['annotations'])):\n",
    "        if data['annotations'][k]['image_id']==observed_image:\n",
    "            image_id_key.append([observed_image])\n",
    "            feature_maps_to_id.append(example_load[observed_image])\n",
    "            caption_to_id.append(re.sub('[^A-Za-z0-9]+',' ',data['annotations'][k]['caption']).lower()) #remove punctuation \n",
    "  \n",
    "print('number of images ',len(image_ids))\n",
    "print('number of captions ',len(caption_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we created three lists, one for the image_id, feature map. and caption. To verify that the indices of each list are aligned, display the image id and caption for one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STRING='%012d' % image_id_key[0][0]\n",
    "exp_image=ndimage.imread(TRAIN_IMAGE_PATH+'COCO_train2014_'+STRING+'.jpg')\n",
    "plt.imshow(exp_image)\n",
    "print('image_id ',image_id_key[:5])\n",
    "print('the captions for this image ')\n",
    "print(caption_to_id[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've aligned image ids, feature vectors, and captions, let's organize the captions for training as we did when predicting next words with RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps=20\n",
    "######################################################################\n",
    "##Create a list of all of the sentences.\n",
    "DatasetWordList=[]\n",
    "for dataset_caption in caption_to_id:\n",
    "        DatasetWordList+=str(dataset_caption).split()\n",
    "#Determine number of distint words \n",
    "distintwords=collections.Counter(DatasetWordList)\n",
    "#Order words \n",
    "count_pairs = sorted(distintwords.items(), key=lambda x: (-x[1], x[0])) #ascending order\n",
    "words, occurence = list(zip(*count_pairs))\n",
    "#DictionaryLength=occurence.index(4) #index for words that occur 4 times or less\n",
    "words=['PAD','UNK','EOS']+list(words)#[:DictionaryLength])\n",
    "word_to_id=dict(zip(words, range(len(words))))\n",
    "#####################  Tokenize Sentence #######################\n",
    "Tokenized=[]\n",
    "for full_words in caption_to_id:\n",
    "        EmbeddedSentence=[word_to_id[word] for word in full_words.split() if word in word_to_id]+[word_to_id['EOS']]\n",
    "        #Pad sentences that are shorter than the number of steps \n",
    "        if len(EmbeddedSentence)<num_steps:\n",
    "            b=[word_to_id['PAD']]*num_steps\n",
    "            b[:len(EmbeddedSentence)]=EmbeddedSentence\n",
    "        if len(EmbeddedSentence)>num_steps:\n",
    "            b=EmbeddedSentence[:num_steps]\n",
    "        if len(b)==EmbeddedSentence:\n",
    "            b=EmeddedSentence\n",
    "        #b=[word_to_id['UNK'] if x>=DictionaryLength else x for x in b] #turn all words used 4 times or less to 'UNK'\n",
    "        #print(b)\n",
    "        Tokenized+=[b]\n",
    "        \n",
    "print(\"Number of words in this dictionary \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenized Sentences\n",
    "Tokenized[::2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains functions for queuing our data and the RNN model. What should the output for each function be? If you need a hint hover [here](#answer2 \"The data_queue function batches the data for us, this needs to return tokenized_caption, input_feature_map. The RNN model should return prediction before the softmax is applied and is defined as pred.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_queue(caption_input,feature_vector,batch_size,):\n",
    "\n",
    "\n",
    "    train_input_queue = tf.train.slice_input_producer(\n",
    "                    [caption_input, np.asarray(feature_vector)],num_epochs=10000,\n",
    "                                    shuffle=True) #False before\n",
    "\n",
    "    ##Set our train data and label input shape for the queue\n",
    "\n",
    "    TrainingInputs=train_input_queue[0]\n",
    "    FeatureVectors=train_input_queue[1]\n",
    "    TrainingInputs.set_shape([num_steps])\n",
    "    FeatureVectors.set_shape([len(feature_vector[0])]) #fc7 is 4096\n",
    "    min_after_dequeue=1000000\n",
    "    capacity = min_after_dequeue + 3 * batch_size \n",
    "    #input_x, target_y\n",
    "    tokenized_caption, input_feature_map = tf.train.batch([TrainingInputs, FeatureVectors],\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 capacity=capacity,\n",
    "                                                 num_threads=6)\n",
    "    return ##FIXME##,##FIXME##\n",
    "    \n",
    "    \n",
    "\n",
    "def rnn_model(Xconcat,input_keep_prob,output_keep_prob,num_layers,num_hidden):\n",
    "#Create a multilayer RNN\n",
    "#reuse=False for training but reuse=True for sharing\n",
    "    layer_cell=[]\n",
    "    for _ in range(num_layers):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_hidden, state_is_tuple=True)\n",
    "        lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,\n",
    "                                          input_keep_prob=input_keep_prob,\n",
    "                                          output_keep_prob=output_keep_prob)\n",
    "        layer_cell.append(lstm_cell)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(layer_cell, state_is_tuple=True)\n",
    "    outputs, last_states = tf.contrib.rnn.static_rnn(\n",
    "        cell=cell,\n",
    "        dtype=tf.float32,\n",
    "        inputs=tf.unstack(Xconcat))\n",
    "\n",
    "    output_reshape=tf.reshape(outputs, [batch_size*(num_steps),num_hidden]) #[12==batch_size*num_steps,num_hidden==12]\n",
    "    pred=tf.matmul(output_reshape, variables_dict[\"weights_mscoco\"]) +variables_dict[\"biases_mscoco\"]\n",
    "    return ##FIXME##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll train our RNN. We'll use the technique outlined in the papers referenced in [1], [2], and [3]: where, at each time step, we'll feed the RNN the current caption AND the feature vector of the image by concatenating each new input with the feature vector. Feel free to examine the papers to dive deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#######################################################################################################\n",
    "# Parameters\n",
    "num_hidden=2048\n",
    "num_steps=num_steps\n",
    "dict_length=len(words)\n",
    "batch_size=4\n",
    "num_layers=2\n",
    "train_lr=0.00001\n",
    "#######################################################################################################\n",
    "TrainingInputs=Tokenized\n",
    "FeatureVectors=feature_maps_to_id\n",
    "\n",
    "## Variables ## \n",
    "# Learning rate placeholder\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "#tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "variables_dict = {\n",
    "    \"weights_mscoco\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32),name=\"weights_mscoco\"),\n",
    "    \"biases_mscoco\": tf.Variable(tf.truncated_normal([dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32), name=\"biases_mscoco\")}\n",
    "\n",
    "\n",
    "tokenized_caption, input_feature_map=data_queue(TrainingInputs,FeatureVectors,batch_size)\n",
    "mscoco_dict=words\n",
    "\n",
    "TrainInput=tf.constant(word_to_id['PAD'],shape=[batch_size,1],dtype=tf.int32)\n",
    "#Pad the beginning of our caption. The first step now only has the image feature vector. Drop the last time step \n",
    "#to timesteps to 20\n",
    "TrainInput=tf.concat([tf.constant(word_to_id['PAD'],shape=[batch_size,1],dtype=tf.int32),\n",
    "                      tokenized_caption],1)[:,:-1]\n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), TrainInput) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "#ImageFeatureTensor=input_feature_map\n",
    "Xconcat=tf.concat([input_feature_map+tf.zeros([num_steps,batch_size,4096]), \n",
    "                     tf.unstack(tf.to_float(X_one_hot),num_steps,1)],2)#[:num_steps,:,:]\n",
    "\n",
    "pred=rnn_model(Xconcat,1.0,1.0,num_layers,num_hidden)\n",
    "\n",
    "\n",
    "#the full caption is the target sentence\n",
    "y_one_hot=tf.unstack(tf.nn.embedding_lookup(np.identity(dict_length), tokenized_caption),num_steps,1) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "\n",
    "y_target_reshape=tf.reshape(y_one_hot,[batch_size*num_steps,dict_length])\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target_reshape))\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(lr,0.9)\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost,aggregation_method = tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gvs]\n",
    "train_op=optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #Load a pretrained network\n",
    "    saver.restore(sess, '/data/mscoco/rnn_layermodel_iter40000')\n",
    "    print('Model restored from file')\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        loss,y_pred,target_caption,_=sess.run([cost,pred,tokenized_caption,train_op],feed_dict={lr:train_lr})\n",
    "\n",
    "        if i% 10==0:\n",
    "            print(\"iteration: \",i, \"loss: \",loss)\n",
    "            \n",
    "    MODEL_NAME='rnn_model_iter'+str(i)\n",
    "    saver.save(sess, MODEL_NAME) \n",
    "    print('saved trained network ',MODEL_NAME)\n",
    "    print(\"Done Training\")\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding some print statements to inspect the size of the input of the to the RNN.  What is the shape of the input to the RNN after the feature vector and caption are merged? Hover [here](#print \"print(Xconcat.shape)\") for a hint.\n",
    "\n",
    "We've asked TensorFlow to measure loss as the 'cross-entropy with logits' of the last layer of our network. This loss measurement is what allows our network to learn and helps us compare performance to other solutions, but you may be more interested in whether our current solution *works* as compared to what *we* know about a satisfactory prediction.\n",
    "\n",
    "We can use the function below to evaluate a single image and its caption from the last batch using the index of the batch. If you need a hint hover [here](#answer3 \"if the batch_size is 4, batch_id may be any value between 0 and 3.\").\n",
    "\n",
    "##### Please note that depending on the status of the neural network at the time it was saved, incomplete, incoherent, and sometimes inappropriate captions could be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_next_predicted_word(batch_id,batch_size,id_of_image,target_caption,predicted_caption,words,PATH):\n",
    "        Target=[words[ind] for ind in target_caption[batch_id]]\n",
    "        Prediction_Tokenized=np.argmax(predicted_caption[batch_id::batch_size],1)\n",
    "        Prediction=[words[ind] for ind in Prediction_Tokenized]\n",
    "        STRING2='%012d' % id_of_image\n",
    "        img=ndimage.imread(PATH+STRING2+'.jpg')\n",
    "        return Target,Prediction,img,STRING2\n",
    "\n",
    "#You can change the batch id to a number between [0 , batch_size-1]\n",
    "batch_id=##FIXME##\n",
    "image_id_for_predicted_caption=[x for x in range(len(Tokenized)) if target_caption[batch_id].tolist()== Tokenized[x]][0]\n",
    "\n",
    "\n",
    "t,p,input_img,string_out=show_next_predicted_word(batch_id,batch_size,image_id_key[image_id_for_predicted_caption][0]\n",
    "                                         ,target_caption,y_pred,words,TRAIN_IMAGE_PATH+'COCO_train2014_')\n",
    "print('Caption')\n",
    "print(t)\n",
    "print('Predicted Words')\n",
    "print(p)\n",
    "plt.imshow(input_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this is a \"next-word\" predictor with the addition of the image as context. Can you think of deployment scenarios where this can be helpful? If so, it works pretty well. However, it doesn't yet solve the challenge we set out to tackle:\n",
    "\n",
    "**Can we generate descriptions of scenes?\"**\n",
    "\n",
    "Let's load our saved network and use it to generate a caption from a validation image:\n",
    "\n",
    "The validation images are stored in /data/mscoco/val2014. A npy file of the feature vectors is stored /data/mscoco/val_vgg_16_fc7_100.npy. For a hint on how to add this look [here](#answer4 \"You can change this parameter to val_load=np.load(/data/mscoco/val_vgg_16_fc7_100.npy).tolist()\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Load and test our test set\n",
    "val_load=np.load('##FIXME##').tolist()\n",
    "val_ids=val_load.keys()\n",
    "\n",
    "#Create 3 lists image_id, feature maps, and captions.\n",
    "val_id_key=[]\n",
    "val_map_to_id=[]\n",
    "val_caption_to_id=[]\n",
    "for observed_image in val_ids:   \n",
    "    val_id_key.append([observed_image])\n",
    "    val_map_to_id.append(val_load[observed_image])\n",
    "    \n",
    "print('number of images ',len(val_ids))\n",
    "print('number of captions ',len(val_map_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will load **only** a feature vector from one of the images in the validation data set and use it with our pretrained network to generate a caption. Use the VALDATA variable to propagate an image through our RNN and generate a caption. You also need to load the network you just created during training. Look here if you need a [hint](#answer5 \"Any of the of the data points in our validation set can be used here. There are 501 captions. Any number between 0 and 100-1 can be used for the VALDATA parameter, such as VALDATA=10. The pretrained network file that you just saved is rnn_model_iter99, insert this string into saver.restore(sess,FILENAME)\").\n",
    "\n",
    "##### Please note that depending on the status of the neural network at the time it was saved, incomplete, incoherent, and sometimes inappropriate captions could be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size=1\n",
    "num_steps=20\n",
    "print_topn=0 #0for do not display \n",
    "printnum0f=3\n",
    "#Choose a image to caption\n",
    "VALDATA=##FIXME##  #ValImage fc7 feature vector\n",
    "\n",
    "variables_dict = {\n",
    "    \"weights_mscoco\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32),name=\"weights_mscoco\"),\n",
    "    \"biases_mscoco\": tf.Variable(tf.truncated_normal([dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32), name=\"biases_mscoco\")}\n",
    "\n",
    "\n",
    "StartCaption=np.zeros([batch_size,num_steps],dtype=np.int32).tolist()\n",
    "\n",
    "CaptionPlaceHolder = tf.placeholder(dtype=tf.int32, shape=(batch_size , num_steps))\n",
    "\n",
    "ValFeatureMap=val_map_to_id[VALDATA]\n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), CaptionPlaceHolder) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "    #ImageFeatureTensor=input_feature_map\n",
    "Xconcat=tf.concat([ValFeatureMap+tf.zeros([num_steps,batch_size,4096]), \n",
    "                            tf.unstack(tf.to_float(X_one_hot),num_steps,1)],2)#[:num_steps,:,:]\n",
    "\n",
    "pred=rnn_model(Xconcat,1.0,1.0,num_layers,num_hidden)\n",
    "pred=tf.nn.softmax(pred)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #Load a pretrained network\n",
    "    saver.restore(sess, '##FIXME##')\n",
    "    print('Model restored from file')\n",
    "    for i in range(num_steps-1):\n",
    "        predict_next_word=sess.run([pred],feed_dict={CaptionPlaceHolder:StartCaption})\n",
    "        INDEX=np.argmax(predict_next_word[0][i])\n",
    "        StartCaption[0][i+1]=INDEX\n",
    "        ##Post N most probable next words at each step\n",
    "        if print_topn !=0:\n",
    "            print(\"Top \",str(printnum0f), \"predictions for the\", str(i+1), \"word in the predicted caption\" )\n",
    "            result_args = np.argsort(predict_next_word[0][i])[-printnum0f:][::-1]\n",
    "            NextWord=[words[x] for x in result_args]\n",
    "            print(NextWord)\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close() \n",
    "\n",
    "STRING2='%012d' % val_id_key[VALDATA][0]\n",
    "img=ndimage.imread('/data/mscoco/val2014/COCO_val2014_'+STRING2+'.jpg')\n",
    "plt.imshow(img)\n",
    "plt.title('COCO_val2014_'+STRING2+'.jpg')\n",
    "PredictedCaption=[words[x] for x in StartCaption[0]]\n",
    "print(\"predicted sentence: \",PredictedCaption[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on which image you choose, you'll get a different caliber of result. While there are different ways to improve results: more training, a bigger/more diverse dataset, better data-network fit, etc, we're going to end this lab with an understanding of this *workflow* and move to attack another challenge: how can we create a caption for video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Free our GPU memory before proceeding to the next part of the lab\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References \n",
    "\n",
    "[1] Donahue, J, et al. \"Long-term recurrent convolutional networks for visual recognition and description.\"     Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n",
    "\n",
    "[2]Vinyals, Oriol, et al. \"Show and tell: Lessons learned from the 2015 mscoco image captioning challenge.\" IEEE transactions on pattern analysis and machine intelligence 39.4 (2017): 652-663.\n",
    "\n",
    "[3] TensorFlow Show and Tell:A Neural Image Caption Generator [example] (https://github.com/tensorflow/models/tree/master/im2txt)\n",
    "\n",
    "[4] Karapthy, A. [NeuralTalk2](https://github.com/karpathy/neuraltalk2)\n",
    "\n",
    "[5]Lin, Tsung-Yi, et al. \"Microsoft coco: Common objects in context.\" European Conference on Computer Vision. Springer International Publishing, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
