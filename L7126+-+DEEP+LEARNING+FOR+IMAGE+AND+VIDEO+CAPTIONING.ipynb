{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](16_DeepLearningInstitute_Logo_R11-RGB_Frame-04.png \"DLI_LOGO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene Generation Description\n",
    "\n",
    "Presented by NVIDIA's Deep Learning Institute\n",
    "\n",
    "Created by Allison Gray and Myrieme Demouth\n",
    "\n",
    "Artificial neural networks are typically structured to make sense out of specific kinds of data: a network designed to identify the main object in an image is different from a network designed to understand the context of a sentence.\n",
    "\n",
    "However, many tasks require multiple types of networks. This lab will combine the outputs of an image classification network with inputs of a natural language processing network to generate descriptions (captions) of images and videos. \n",
    "\n",
    "In this lab, attendees will learn about how to use recurrent neural networks for character and sentence generation before using TensorFlow to combine workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Task 1 Recurrent Neural Networks](wordprediction/RecurrentNeuralNetwork.ipynb)\n",
    "In this part of the lab, we will demonstrate the power of RNNs with word generation examples. Participants will explore and experiment with RNNs to learn how to configure and use them to generate sentences. Captions from MSCOCO dataset will be used to train an RNN with different network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Task 2 Image Captioning](imagecaptioning/MSCOCO_Captioning.ipynb)\n",
    "Image captioning can be performed by combining a CNN with an RNN. Attendees will get hands-on experience with combining data from CNNs with RNNs. The MSCOCO images and captions will be used to train and finetune a network to generate captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Task 3 -  Video Captioning](videocaptioning/VideoCaptioning.ipynb)\n",
    "In this last part, we will combine all the things we learned about in the previous three tasks to generate captions about *video* clips. If you'd like to try to build it yourself, below is a version that walks you through finding the mean image and then leaves space for you to apply what you've learned in the workshop so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Task 3 Challenge - Video Captioning Blank - Optional](videocaptioning/VideoCaptioningBuild.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
